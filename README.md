# candle-vllm

Efficent platform for inference and serving local LLMs including an OpenAI compatible API server.

## Resources
- Python implementation: [`vllm-project`](https://github.com/vllm-project/vllm)